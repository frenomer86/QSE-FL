{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf11a999-b0e5-4dca-a359-a6367008f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, mean_absolute_error \n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, Multiply, concatenate, Input, BatchNormalization, ReLU, Add, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import keras.backend as K\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Imports\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from glob import glob\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.models import load_model\n",
    "from keras.losses import binary_crossentropy \n",
    "from tensorflow.keras.layers import ReLU, Add  \n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Sequential \n",
    "from dilithium_py.dilithium import Dilithium2, Dilithium3, Dilithium5\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "import matplotlib.cm as cm \n",
    "import tenseal as ts\n",
    "import random\n",
    "import time\n",
    " \n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(seed=42)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb7e2bc-e055-4bd5-98af-32999b708788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mock training, validation, and test data\n",
    "\n",
    "# Image size and other constants\n",
    "IMG_SIZE = 240\n",
    "batch_size = 32\n",
    "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, 3) \n",
    "num_clients = 5\n",
    "epochs = 100\n",
    " \n",
    "\n",
    "train_image_dir = \"/home/freno/Desktop/THESIS/JAJA-TA/images\"\n",
    "train_mask_dir = \"/home/freno/Desktop/THESIS/JAJA-TA/masks\"\n",
    "\n",
    "\n",
    "test_image_dir = \"/home/freno/Desktop/THESIS/JAJA-TA/TEST_ALL/general_test/images\"\n",
    "test_mask_dir = \"/home/freno/Desktop/THESIS/JAJA-TA/TEST_ALL/general_test/masks\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c17f302e-0706-499b-be46-9c8dc4a7c814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 22:17:06.276421: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-13 22:17:06.338238: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-13 22:17:06.338273: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-13 22:17:06.338312: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-13 22:17:06.373560: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-13 22:17:06.374569: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-13 22:17:07.994616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, image_paths, mask_paths, target_shape=(IMG_SIZE, IMG_SIZE), augment=False):\n",
    "        self.image_paths = sorted(image_paths)\n",
    "        self.mask_paths = sorted(mask_paths)\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices((self.image_paths, self.mask_paths))\n",
    "        self.AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "        self.target_shape = target_shape\n",
    "        self.augment = augment  # Set augment flag\n",
    "\n",
    "    @tf.function\n",
    "    def parse_images(self, image_path, mask_path):\n",
    "        # Read and preprocess the image\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.io.decode_jpeg(image, channels=3)\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = tf.image.resize(image, self.target_shape) / 255.\n",
    "\n",
    "        # Read and preprocess the mask\n",
    "        mask = tf.io.read_file(mask_path)\n",
    "        mask = tf.io.decode_jpeg(mask, channels=1)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        mask = tf.image.resize(mask, self.target_shape) / 255.\n",
    "\n",
    "        # Apply data augmentation if augment flag is True\n",
    "        if self.augment:\n",
    "            image, mask = self.augment_image_mask(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    @tf.function \n",
    "    def data_generator(self, batch_size=batch_size, shuffle=True, repeat=False):\n",
    "        dataset = self.dataset\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=len(self.image_paths))\n",
    "    \n",
    "        dataset = dataset.map(self.parse_images, num_parallel_calls=self.AUTOTUNE)\n",
    "    \n",
    "        if repeat:\n",
    "            dataset = dataset.repeat()  # Use repeat only for training\n",
    "    \n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(buffer_size=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def augment_image_mask(self, image, mask):\n",
    "        \"\"\"Apply data augmentation to image and mask.\"\"\"\n",
    "        # Random flip (horizontal and/or vertical)\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            image = tf.image.flip_left_right(image)\n",
    "            mask = tf.image.flip_left_right(mask)\n",
    "\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            image = tf.image.flip_up_down(image)\n",
    "            mask = tf.image.flip_up_down(mask)\n",
    "\n",
    "        # Random rotation (90 degrees increments)\n",
    "        k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
    "        image = tf.image.rot90(image, k)\n",
    "        mask = tf.image.rot90(mask, k)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "def prepare_federated_data(image_dir, mask_dir, num_clients=5, target_shape=(IMG_SIZE, IMG_SIZE), augment=True):\n",
    "    # Get image and mask paths\n",
    "    image_paths = sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    mask_paths = sorted([os.path.join(mask_dir, fname) for fname in os.listdir(mask_dir) if fname.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    \n",
    "    # Initialize the DataLoader with all paths\n",
    "    total_loader = DataLoader(image_paths, mask_paths, target_shape=target_shape, augment=augment)\n",
    "    \n",
    "    # Get the total dataset\n",
    "    total_dataset = total_loader.dataset\n",
    "    total_size = len(image_paths)\n",
    "    \n",
    "    # Shuffle the dataset before splitting\n",
    "    total_dataset = total_dataset.shuffle(buffer_size=total_size)\n",
    "    \n",
    "    # Split the dataset into num_clients subsets\n",
    "    client_datasets = []\n",
    "    split_size = total_size // num_clients\n",
    "    for i in range(num_clients):\n",
    "        start_idx = i * split_size\n",
    "        end_idx = (i + 1) * split_size if i != num_clients - 1 else total_size\n",
    "        \n",
    "        client_image_paths = image_paths[start_idx:end_idx]\n",
    "        client_mask_paths = mask_paths[start_idx:end_idx]\n",
    "        \n",
    "        client_loader = DataLoader(client_image_paths, client_mask_paths, target_shape=target_shape, augment=augment)\n",
    "        client_datasets.append(client_loader)\n",
    "\n",
    "    return client_datasets\n",
    "\n",
    "# Prepare the federated datasets for training\n",
    "train_client_datasets = prepare_federated_data(train_image_dir, train_mask_dir, num_clients=num_clients, augment=True)\n",
    "test_client_datasets = prepare_federated_data(test_image_dir, test_mask_dir, num_clients=num_clients, augment=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a477c5-879a-49bd-8157-03fa3f3b8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def conv_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    \"\"\"Basic convolutional block with two Conv2D layers and batch normalization.\"\"\"\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    c = BatchNormalization()(c)\n",
    "    return c\n",
    "\n",
    "def down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    \"\"\"Downsampling block: Conv2D + Conv2D + MaxPooling2D.\"\"\"\n",
    "    c = conv_block(x, filters, kernel_size, padding, strides)\n",
    "    p = MaxPooling2D((2, 2), (2, 2))(c)\n",
    "    return c, p\n",
    "\n",
    "def up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    \"\"\"Upsampling block: Upsample + Conv2D + Concatenate with skip connections.\"\"\"\n",
    "    us = UpSampling2D((2, 2))(x)\n",
    "    concat = Concatenate()([us, skip])\n",
    "    c = conv_block(concat, filters, kernel_size, padding, strides)\n",
    "    return c\n",
    "\n",
    "def UNet():\n",
    "    f = [4, 8, 16, 32, 64]  # Filter sizes for each level\n",
    "    inputs = Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "    # Downsampling path (Encoder)\n",
    "    c1, t1 = down_block(inputs, f[0])\n",
    "    c2, t2 = down_block(t1, f[1])\n",
    "    c3, t3 = down_block(t2, f[2])\n",
    "    c4, t4 = down_block(t3, f[3])\n",
    "\n",
    "    # Bottleneck\n",
    "    bn = conv_block(t4, f[4])\n",
    "\n",
    "    # Nested connections (U-Net++)\n",
    "    # Dense skip connections between the decoder and encoder\n",
    "    u1_0 = up_block(bn, c4, f[3])\n",
    "    u2_0 = up_block(u1_0, c3, f[2])\n",
    "    u3_0 = up_block(u2_0, c2, f[1])\n",
    "    u4_0 = up_block(u3_0, c1, f[0])\n",
    "\n",
    "    # Extra skip connections for U-Net++\n",
    "    u2_1 = up_block(u1_0, u2_0, f[2])\n",
    "    u3_1 = up_block(u2_1, u3_0, f[1])\n",
    "    u4_1 = up_block(u3_1, u4_0, f[0])\n",
    "\n",
    "    u3_2 = up_block(u2_1, u3_1, f[1])\n",
    "    u4_2 = up_block(u3_2, u4_1, f[0])\n",
    "\n",
    "    u4_3 = up_block(u3_2, u4_2, f[0])\n",
    "\n",
    "    # Final output layer\n",
    "    outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4_3)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs, outputs) \n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the U-Net model\n",
    "global_model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e5235-f3af-49c9-8d84-eb66f0fa3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Train on Each Client\n",
    "def train_on_client(client_data, global_model, epochs=5, batch_size=32):\n",
    "    # Clone the global model for client-specific training\n",
    "    client_model = tf.keras.models.clone_model(global_model)\n",
    "    client_model.set_weights(global_model.get_weights())  # Initialize with global model weights\n",
    "\n",
    "    client_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                         loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "    client_generator = client_data.data_generator(batch_size=batch_size)\n",
    "    \n",
    "    # Fit the client model and get the training loss\n",
    "    history = client_model.fit(client_generator, epochs=epochs, steps_per_epoch=len(client_data.image_paths) // batch_size)\n",
    "\n",
    "    # Return weight updates and training loss\n",
    "    weight_updates = [client_model.get_weights()[i] - global_model.get_weights()[i] \n",
    "                      for i in range(len(global_model.get_weights()))]\n",
    "    return weight_updates, history.history['loss'][-1]  # Return final loss of the last epoch\n",
    "    \n",
    "\n",
    "# Federated Averaging to Aggregate the Weights\n",
    "def aggregate_models(global_model, client_weight_updates, num_clients):\n",
    "    # Average the weight updates from all clients\n",
    "    averaged_weight_updates = [np.mean([client_weight_updates[i][j] for i in range(num_clients)], axis=0) \n",
    "                               for j in range(len(global_model.get_weights()))]\n",
    "\n",
    "    # Apply the averaged weight updates to the global model\n",
    "    new_global_weights = [global_model.get_weights()[i] + averaged_weight_updates[i] \n",
    "                          for i in range(len(global_model.get_weights()))]\n",
    "    global_model.set_weights(new_global_weights)\n",
    "    \n",
    "\n",
    "# Federated Learning Process with Loss Tracking\n",
    "def federated_training(train_client_datasets, global_model, epochs=10, num_clients=5, batch_size=32):\n",
    "    \"\"\"\n",
    "    Federated training with client weight updates and loss tracking.\n",
    "\n",
    "    Args:\n",
    "        train_client_datasets: List of DataLoader instances for each client.\n",
    "        global_model: The global model to be trained.\n",
    "        epochs: Number of global training epochs.\n",
    "        num_clients: Number of clients.\n",
    "        batch_size: Batch size for training.\n",
    "\n",
    "    Returns:\n",
    "        Updated global model and list of global training losses.\n",
    "    \"\"\"\n",
    "    global_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                         loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "    global_losses = []  # Track the global loss after each epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Global Training Epoch {epoch + 1}/{epochs}\")\n",
    "        client_weight_updates = []\n",
    "        epoch_loss = 0  # To accumulate loss from all clients\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            print(f\"Training on client {i + 1}/{num_clients}\")\n",
    "            # Train the client and collect weight updates (deltas) and loss\n",
    "            weight_updates, client_loss = train_on_client(train_client_datasets[i], global_model, epochs=1, batch_size=batch_size)\n",
    "            client_weight_updates.append(weight_updates)\n",
    "            epoch_loss += client_loss  # Accumulate client loss\n",
    "\n",
    "        # Aggregate the model using weight updates\n",
    "        aggregate_models(global_model, client_weight_updates, num_clients)\n",
    "        print(\"Updated global model.\")\n",
    "\n",
    "        # Average the loss across clients and store for plotting\n",
    "        avg_epoch_loss = epoch_loss / num_clients\n",
    "        global_losses.append(avg_epoch_loss)\n",
    "        print(f\"Average Loss for Epoch {epoch + 1}: {avg_epoch_loss}\")\n",
    "\n",
    "    return global_model, global_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef4b864-be40-4732-8ebb-4bb9dcc5613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new model for tandard FedAvg\n",
    "global_fedavg = UNet()\n",
    "\n",
    "# Run standard FedAvg\n",
    "print(\"Running Standard FedAvg...\") \n",
    "start_time = time.time()\n",
    "# Train the global model with FedAvg \n",
    "fedavg_model, fedavg_loss =  federated_training(train_client_datasets, \n",
    "                                                global_model, epochs=epochs, \n",
    "                                                num_clients=num_clients, batch_size=batch_size)\n",
    "end_time = time.time()\n",
    "\n",
    "# Store training time for Standard FedAvg\n",
    "fl_training_times = end_time - start_time\n",
    "\n",
    "print(f\"Standard FedAvg training time: {fl_training_times:.2f} seconds\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e73edd-f3d1-4f4d-b821-b6adc5e45dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "# Dilithium-based secure key generation, signing, and verification\n",
    "class DilithiumSecurity:\n",
    "    def __init__(self):\n",
    "        self.pk, self.sk = Dilithium2.keygen()\n",
    "\n",
    "    def sign(self, msg):\n",
    "        return Dilithium2.sign(self.sk, msg)\n",
    "\n",
    "    def verify(self, msg, sig, pk):\n",
    "        return Dilithium2.verify(pk, msg, sig)\n",
    "\n",
    "\n",
    "# Fully Homomorphic Encryption with Chunking\n",
    "class FullyHomomorphicEncryption:\n",
    "    def __init__(self, poly_modulus_degree=16384):\n",
    "        self.context = ts.context(\n",
    "            ts.SCHEME_TYPE.CKKS,\n",
    "            poly_modulus_degree=poly_modulus_degree,\n",
    "            coeff_mod_bit_sizes=[60, 40, 60]\n",
    "        )\n",
    "        self.context.generate_galois_keys()\n",
    "        self.context.global_scale = 2 ** 40\n",
    "\n",
    "    def encrypt_tensor_in_chunks(self, tensor, chunk_size):\n",
    "        flat_tensor = tensor.flatten()\n",
    "        chunks = [flat_tensor[i:i + chunk_size] for i in range(0, len(flat_tensor), chunk_size)]\n",
    "        encrypted_chunks = [ts.ckks_vector(self.context, chunk) for chunk in chunks]\n",
    "        return encrypted_chunks\n",
    "\n",
    "    def decrypt_tensor_in_chunks(self, encrypted_chunks, original_shape):\n",
    "        decrypted_chunks = [chunk.decrypt() for chunk in encrypted_chunks]\n",
    "        flat_tensor = np.concatenate(decrypted_chunks)\n",
    "        return np.reshape(flat_tensor, original_shape)\n",
    "\n",
    "    def add_encrypted_chunks(self, enc1_chunks, enc2_chunks):\n",
    "        return [enc1_chunks[i] + enc2_chunks[i] for i in range(len(enc1_chunks))]\n",
    "\n",
    "    def multiply_encrypted_chunks(self, enc_chunks, scalar):\n",
    "        return [chunk * scalar for chunk in enc_chunks]\n",
    "\n",
    "\n",
    "# Training on a client\n",
    "def train_on_client(client_data, global_model, epochs=5, batch_size=32):\n",
    "    client_model = tf.keras.models.clone_model(global_model)\n",
    "    client_model.set_weights(global_model.get_weights())\n",
    "    client_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                         loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "    client_generator = client_data.data_generator(batch_size=batch_size)\n",
    "    history = client_model.fit(client_generator, epochs=epochs,\n",
    "                               steps_per_epoch=len(client_data.image_paths) // batch_size)\n",
    "\n",
    "    weight_updates = [client_model.get_weights()[i] - global_model.get_weights()[i]\n",
    "                      for i in range(len(global_model.get_weights()))]\n",
    "    return weight_updates, history.history['loss'][-1]\n",
    "\n",
    "\n",
    "# QSE-FL Training with FHE and Secure Aggregation\n",
    "def qse_fl_training(train_client_datasets, global_model, epochs=10, num_clients=5, batch_size=32, chunk_size=8192):\n",
    "    global_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                         loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "    global_losses = []\n",
    "    server_security = DilithiumSecurity()\n",
    "    client_security = [DilithiumSecurity() for _ in range(num_clients)]\n",
    "    fhe = FullyHomomorphicEncryption()\n",
    "\n",
    "    # Server key pair generation\n",
    "    server_pk, server_sk = Dilithium2.keygen()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Global Training Epoch {epoch + 1}/{epochs}\")\n",
    "        client_weight_updates = []\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # Server signs the global model\n",
    "        global_model_weights = global_model.get_weights()\n",
    "        signed_model = server_security.sign(str(global_model_weights).encode())\n",
    "\n",
    "        # Broadcast signed global model to all clients\n",
    "        for i in range(num_clients):\n",
    "            print(f\"Training on client {i + 1}/{num_clients}\")\n",
    "\n",
    "            # Client verifies the signed model\n",
    "            if not client_security[i].verify(str(global_model_weights).encode(), signed_model, server_security.pk):\n",
    "                print(f\"Client {i + 1} rejected the server model (verification failed).\")\n",
    "                break\n",
    "\n",
    "            # Train on client data\n",
    "            weight_updates, client_loss = train_on_client(train_client_datasets[i], global_model, epochs=1, batch_size=batch_size)\n",
    "            epoch_loss += client_loss\n",
    "\n",
    "            # Client signs the local updates\n",
    "            signed_updates = client_security[i].sign(str(weight_updates).encode())\n",
    "\n",
    "            # Verify the signed updates before sending them to the server\n",
    "            if client_security[i].verify(str(weight_updates).encode(), signed_updates, client_security[i].pk):\n",
    "                # Encrypt weight updates using FHE\n",
    "                encrypted_weight_updates = [\n",
    "                    fhe.encrypt_tensor_in_chunks(np.array(w), chunk_size) for w in weight_updates\n",
    "                ]\n",
    "                client_weight_updates.append(encrypted_weight_updates)\n",
    "\n",
    "        # Secure Aggregation using FHE in chunks\n",
    "        if client_weight_updates:\n",
    "            encrypted_aggregated_updates = []\n",
    "            for j in range(len(global_model.get_weights())):\n",
    "                # Sum up encrypted chunks\n",
    "                enc_sum = client_weight_updates[0][j]\n",
    "                for k in range(1, len(client_weight_updates)):\n",
    "                    enc_sum = fhe.add_encrypted_chunks(enc_sum, client_weight_updates[k][j])\n",
    "\n",
    "                # Average the sum\n",
    "                encrypted_avg = fhe.multiply_encrypted_chunks(enc_sum, 1 / num_clients)\n",
    "                encrypted_aggregated_updates.append(encrypted_avg)\n",
    "\n",
    "            # Decrypt the aggregated updates\n",
    "            decrypted_updates = [\n",
    "                fhe.decrypt_tensor_in_chunks(enc_update, global_model.get_weights()[j].shape)\n",
    "                for j, enc_update in enumerate(encrypted_aggregated_updates)\n",
    "            ]\n",
    "\n",
    "            # Update the global model\n",
    "            new_global_weights = [global_model.get_weights()[i] + decrypted_updates[i] for i in range(len(global_model.get_weights()))]\n",
    "            global_model.set_weights(new_global_weights)\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / num_clients if num_clients > 0 else 0\n",
    "        global_losses.append(avg_epoch_loss)\n",
    "        print(f\"Average Loss for Epoch {epoch + 1}: {avg_epoch_loss}\")\n",
    "\n",
    "    return global_model, global_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebec788-9721-45a8-8b19-f66521304b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new model for QKD and FHE\n",
    "global_qcefl = UNet()\n",
    "    \n",
    "# Run FedAvg with QKD and FHE\n",
    "print(\"QSE-FHE...\")\n",
    "    \n",
    "start_time = time.time()\n",
    "# Train the global model with FedAvg\n",
    "qce_fl_model, qce_loss = qse_fl_training(train_client_datasets, \n",
    "                                                global_model, epochs=epochs, \n",
    "                                                num_clients=num_clients, batch_size=batch_size )\n",
    "end_time = time.time()\n",
    "\n",
    "# Store training time for FedAvg with QKD and FHE\n",
    "qce_training_times = end_time - start_time\n",
    "\n",
    "print(f\"QCE-FL training time: {qce_training_times:.2f} seconds\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9770b-1814-494a-8856-9351f577abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "epochs = range(1, len(qce_loss) + 1)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(epochs, fedavg_loss, label='FedAvg Loss', marker='o', color='blue')\n",
    "plt.plot(epochs, qce_loss, label='QCE-FL Loss', marker='o', color='red')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "#plt.title('Training Loss Comparison: QCE-FL vs FedAvg')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0016ec29-8fcc-4a69-a54e-38ab8386c640",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def plot_computational_burden(fl_training_times, qce_training_times):\n",
    "    # Define labels and values\n",
    "    labels = ['FedAvg', 'QCE-FL']\n",
    "    values = [fl_training_times, qce_training_times]\n",
    "    \n",
    "    # Define colors\n",
    "    colors = ['#D3D3D3', '#FFFFE0']  # Light ash and light yellow\n",
    "\n",
    "    # Create the bar chart\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(labels, values, color=colors)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Computational Burden: FedAvg vs QCE-FL')\n",
    "    plt.ylabel('Training Time (seconds)')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_computational_burden(fl_training_times, qce_training_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c291011c-b296-47d8-a280-4c35eae05586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a combined test dataset\n",
    "def create_combined_test_dataset(test_client_datasets):\n",
    "    combined_image_paths = []\n",
    "    combined_mask_paths = []\n",
    "\n",
    "    # Iterate over each client's test dataset to combine the paths\n",
    "    for client_data in test_client_datasets:\n",
    "        combined_image_paths.extend(client_data.image_paths)\n",
    "        combined_mask_paths.extend(client_data.mask_paths)\n",
    "\n",
    "    # Create a new DataLoader for the combined test dataset\n",
    "    combined_dataset = DataLoader(combined_image_paths, combined_mask_paths, augment=False)\n",
    "    return combined_dataset\n",
    "\n",
    " \n",
    "# Create the combined test dataset\n",
    "combined_test_dataset = create_combined_test_dataset(test_client_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54799b2e-74c0-4f8a-b56e-ec4f934a63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract ground truth masks from the combined test dataset\n",
    "def extract_ground_truth_masks(data_loader):\n",
    "    masks = []\n",
    "\n",
    "    for image_batch, mask_batch in data_loader.data_generator(batch_size=1, shuffle=False):\n",
    "        masks.append(mask_batch.numpy())  # Convert tensor to numpy array\n",
    "\n",
    "    # Stack the masks to form a single array\n",
    "    return np.vstack(masks)  # Use vstack to concatenate along the first axis\n",
    "\n",
    "# Get ground truth masks\n",
    "ground_truth_masks = extract_ground_truth_masks(combined_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8743e35e-7ae0-4cff-9ad4-beb4a8e970ed",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def dice_score(predictions, masks, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the DICE score for the given predictions and ground truth masks.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions (numpy array).\n",
    "        masks: Ground truth masks (numpy array).\n",
    "        threshold: Threshold to binarize predictions.\n",
    "    \n",
    "    Returns:\n",
    "        DICE score.\n",
    "    \"\"\"\n",
    "    predictions_binary = (predictions > threshold).astype(np.float32)\n",
    "    intersection = np.sum(predictions_binary * masks)\n",
    "    return 2 * intersection / (np.sum(predictions_binary) + np.sum(masks) + 1e-8)  # Add epsilon to avoid division by zero\n",
    "\n",
    "# Predicting on the test datasets\n",
    "def predict_on_test(test_data, global_model, batch_size=32):\n",
    "    test_generator = test_data.data_generator(batch_size=batch_size, shuffle=False)\n",
    "    predictions = global_model.predict(test_generator)\n",
    "    return predictions\n",
    "\n",
    "\n",
    " \n",
    "# Get predictions for both models\n",
    "predictions_fedavg = predict_on_test(combined_test_dataset, fedavg_model)\n",
    "predictions_qce_fl = predict_on_test(combined_test_dataset, qce_fl_model)\n",
    "\n",
    "# Calculate DICE scores\n",
    "dice_score_fedavg = dice_score(predictions_fedavg, ground_truth_masks)\n",
    "dice_score_qce_fl = dice_score(predictions_qce_fl, ground_truth_masks)\n",
    "\n",
    "# Plot DICE scores\n",
    "models = ['FedAvg', 'QCE-FL']\n",
    "dice_scores = [dice_score_fedavg, dice_score_qce_fl]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(models, dice_scores, color=['lightyellow', 'lightcoral'])\n",
    "plt.ylabel('DICE Score')\n",
    "#plt.title('DICE Scores of the Models')\n",
    "plt.ylim(0, 1)  # DICE scores range from 0 to 1\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279a60a-cf3e-449d-ac28-36bcda846649",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_models = {\n",
    "    'FedAvg': fedavg_model,\n",
    "    'QCE-FL': qce_fl_model\n",
    "}\n",
    "\n",
    "# Assuming combined_test_dataset is your DataLoader instance\n",
    "# Make sure you get the original images in the correct format\n",
    "original_images = [test_data[0] for test_data in combined_test_dataset.data_generator(batch_size=len(combined_test_dataset.image_paths), shuffle=False)]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bb11a-1f5b-4321-b6e6-c64dcee0c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def visualize_results(images, ground_truth, models, num_samples=2):\n",
    "    \"\"\"\n",
    "    Visualizes the segmentation results for the specified number of samples.\n",
    "\n",
    "    Args:\n",
    "        images (list): List of original images.\n",
    "        ground_truth (np.ndarray): Array of ground truth masks.\n",
    "        models (dict): Dictionary of models to evaluate.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "    \"\"\"\n",
    "    num_models = len(models)\n",
    "\n",
    "    # Determine how many samples to visualize based on available data\n",
    "    num_samples = min(num_samples, len(images))\n",
    "\n",
    "    # Create a figure with adjusted spacing\n",
    "    fig, axes = plt.subplots(\n",
    "        num_samples, \n",
    "        num_models + 2,  # 2 for original and ground truth, plus models\n",
    "        figsize=(15, 5 * num_samples), \n",
    "        gridspec_kw={'wspace': 0.01, 'hspace': 0.1}\n",
    "    )\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Convert image to numpy array (assuming images are still Tensors)\n",
    "        image_np = images[i].numpy() if isinstance(images[i], tf.Tensor) else images[i]\n",
    "        ground_truth_np = ground_truth[i]  # No need to call .numpy() here\n",
    "\n",
    "        # Display original image\n",
    "        ax = axes[i, 0] if num_samples > 1 else axes[0]\n",
    "        ax.imshow(image_np[0], cmap='gray')  # Display the first image in the batch\n",
    "        ax.axis('off')\n",
    "        if i == num_samples - 1:\n",
    "            ax.set_title(\"Image\", y=-0.2, fontsize=14)\n",
    "        \n",
    "        # Display ground truth mask overlaid on the original image\n",
    "        ax = axes[i, 1] if num_samples > 1 else axes[1]\n",
    "        ax.imshow(image_np[0], cmap='gray')  # Show original image\n",
    "        ax.imshow(ground_truth_np.squeeze(), cmap='jet', alpha=0.5)  # Overlay mask\n",
    "        ax.axis('off')\n",
    "        if i == num_samples - 1:\n",
    "            ax.set_title(\"GT\", y=-0.2, fontsize=14)\n",
    "        \n",
    "        # Display predicted masks overlaid on the original image for each model\n",
    "        for j, (model_name, model) in enumerate(models.items()):\n",
    "            # Predict and ensure it is a numpy array\n",
    "            predicted_mask = model.predict(tf.expand_dims(image_np[0], axis=0), verbose=0)[0]\n",
    "            if isinstance(predicted_mask, tf.Tensor):\n",
    "                predicted_mask = predicted_mask.numpy()\n",
    "            \n",
    "            ax = axes[i, j + 2] if num_samples > 1 else axes[j + 2]\n",
    "            ax.imshow(image_np[0], cmap='gray')  # Show original image\n",
    "            ax.imshow(predicted_mask.squeeze(), cmap='jet', alpha=0.5)  # Overlay predicted mask\n",
    "            ax.axis('off')\n",
    "            if i == num_samples - 1:\n",
    "                ax.set_title(model_name, y=-0.2, fontsize=14)\n",
    "            \n",
    "    # Adjust subplot spacing\n",
    "    plt.subplots_adjust(wspace=0.1, top=0.9, hspace=0.1)\n",
    "    plt.show()\n",
    "\n",
    " \n",
    "# Call the function to visualize results\n",
    "visualize_results(original_images, ground_truth_masks, fl_models, num_samples=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0da3cb-d3a7-42b0-a1fe-df90275b62d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a83c5d-782b-43ec-9a24-3d3628e6b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e883da-2c8e-405b-b15c-e2c4687fbf8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
